# Models - LinkML Data
# Conforms to: ccf_models.yaml

models:
  # ==========================================================================
  # Qwen Family
  # ==========================================================================

  - id: ccf:model:qwen2.5-coder-7b
    label: Qwen2.5-Coder-7B-Instruct
    description: 7B code-specialized model from Qwen 2.5 series
    member_of_family: ccf:family:qwen
    has_capability:
      - ccf:capability:code-generation
      - ccf:capability:code-completion
      - ccf:capability:instruction-following
    parameter_count: "7B"
    context_length: 131072
    fits_tier:
      - ccf:tier:gpu-8gb
    recommended_quantization: q4_k_m
    license: apache-2.0
    status: active
    xref_ollama: "qwen2.5-coder:7b-instruct"
    xref_huggingface: Qwen/Qwen2.5-Coder-7B-Instruct
    cc_forge_notes: Primary coding model for Tier 1

  - id: ccf:model:qwen2.5-coder-32b
    label: Qwen2.5-Coder-32B-Instruct
    description: 32B code-specialized model, best open-source coder
    member_of_family: ccf:family:qwen
    has_capability:
      - ccf:capability:code-generation
      - ccf:capability:code-completion
      - ccf:capability:instruction-following
    parameter_count: "32B"
    context_length: 131072
    fits_tier:
      - ccf:tier:cpu-64gb
    recommended_quantization: q4_k_m
    license: apache-2.0
    status: active
    xref_ollama: "qwen2.5-coder:32b-instruct"
    xref_huggingface: Qwen/Qwen2.5-Coder-32B-Instruct
    cc_forge_notes: Use when 7B produces incorrect code

  - id: ccf:model:qwen2.5-7b
    label: Qwen2.5-7B-Instruct
    description: 7B general-purpose model from Qwen 2.5 series
    member_of_family: ccf:family:qwen
    has_capability:
      - ccf:capability:chat
      - ccf:capability:instruction-following
      - ccf:capability:long-context
    parameter_count: "7B"
    context_length: 131072
    fits_tier:
      - ccf:tier:gpu-8gb
    recommended_quantization: q4_k_m
    license: apache-2.0
    status: active
    xref_ollama: "qwen2.5:7b-instruct"
    xref_huggingface: Qwen/Qwen2.5-7B-Instruct

  - id: ccf:model:qwq-32b
    label: QwQ-32B
    description: 32B reasoning model with chain-of-thought capability
    member_of_family: ccf:family:qwen
    has_capability:
      - ccf:capability:reasoning-cot
      - ccf:capability:reasoning-math
      - ccf:capability:code-generation
    parameter_count: "32B"
    context_length: 32768
    fits_tier:
      - ccf:tier:cpu-64gb
    recommended_quantization: q4_k_m
    license: apache-2.0
    status: active
    xref_ollama: "qwq:32b"
    xref_huggingface: Qwen/QwQ-32B
    cc_forge_notes: Best reasoning at manageable size

  # ==========================================================================
  # DeepSeek Family
  # ==========================================================================

  - id: ccf:model:deepseek-r1
    label: DeepSeek-R1
    description: 671B MoE reasoning model rivaling OpenAI o1
    member_of_family: ccf:family:deepseek
    has_capability:
      - ccf:capability:reasoning-cot
      - ccf:capability:reasoning-math
      - ccf:capability:code-generation
    parameter_count: "671B"
    context_length: 131072
    fits_tier:
      - ccf:tier:api
    license: mit
    status: active
    xref_huggingface: deepseek-ai/DeepSeek-R1
    cc_forge_notes: Too large for local, use distills or API

  - id: ccf:model:deepseek-r1-7b
    label: DeepSeek-R1-Distill-7B
    description: 7B distilled reasoning model from DeepSeek R1
    member_of_family: ccf:family:deepseek
    derived_from: ccf:model:deepseek-r1
    has_capability:
      - ccf:capability:reasoning-cot
      - ccf:capability:reasoning-math
    parameter_count: "7B"
    context_length: 65536
    fits_tier:
      - ccf:tier:gpu-8gb
    recommended_quantization: q4_k_m
    license: mit
    status: active
    xref_ollama: "deepseek-r1:7b"
    xref_huggingface: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
    cc_forge_notes: Reasoning capability at Tier 1 size

  - id: ccf:model:deepseek-r1-32b
    label: DeepSeek-R1-Distill-32B
    description: 32B distilled reasoning model from DeepSeek R1
    member_of_family: ccf:family:deepseek
    derived_from: ccf:model:deepseek-r1
    has_capability:
      - ccf:capability:reasoning-cot
      - ccf:capability:reasoning-math
    parameter_count: "32B"
    context_length: 65536
    fits_tier:
      - ccf:tier:cpu-64gb
    recommended_quantization: q4_k_m
    license: mit
    status: active
    xref_ollama: "deepseek-r1:32b"
    xref_huggingface: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B

  # ==========================================================================
  # Llama Family
  # ==========================================================================

  - id: ccf:model:llama-3.1-8b
    label: Llama-3.1-8B-Instruct
    description: 8B general-purpose model from Meta
    member_of_family: ccf:family:llama
    has_capability:
      - ccf:capability:chat
      - ccf:capability:instruction-following
      - ccf:capability:long-context
    parameter_count: "8B"
    context_length: 131072
    fits_tier:
      - ccf:tier:gpu-8gb
    recommended_quantization: q4_k_m
    license: llama
    status: active
    xref_ollama: "llama3.1:8b-instruct"
    xref_huggingface: meta-llama/Llama-3.1-8B-Instruct
    cc_forge_notes: Good general-purpose baseline

  - id: ccf:model:llama-3.3-70b
    label: Llama-3.3-70B-Instruct
    description: 70B model with 405B-level quality
    member_of_family: ccf:family:llama
    has_capability:
      - ccf:capability:chat
      - ccf:capability:instruction-following
      - ccf:capability:long-context
    parameter_count: "70B"
    context_length: 131072
    fits_tier:
      - ccf:tier:cpu-64gb
    recommended_quantization: q4_k_m
    license: llama
    status: active
    xref_ollama: "llama3.3:70b-instruct"
    xref_huggingface: meta-llama/Llama-3.3-70B-Instruct
    cc_forge_notes: Best quality when you can wait

  # ==========================================================================
  # Mistral Family
  # ==========================================================================

  - id: ccf:model:mistral-7b
    label: Mistral-7B-Instruct
    description: Efficient 7B model with sliding window attention
    member_of_family: ccf:family:mistral
    has_capability:
      - ccf:capability:chat
      - ccf:capability:instruction-following
    parameter_count: "7B"
    context_length: 32768
    fits_tier:
      - ccf:tier:gpu-8gb
    recommended_quantization: q4_k_m
    license: apache-2.0
    status: active
    xref_ollama: "mistral:7b-instruct"
    xref_huggingface: mistralai/Mistral-7B-Instruct-v0.3

  - id: ccf:model:codestral-22b
    label: Codestral-22B
    description: 22B code model with 256K context and FIM support
    member_of_family: ccf:family:mistral
    has_capability:
      - ccf:capability:code-generation
      - ccf:capability:code-completion
    parameter_count: "22B"
    context_length: 262144
    fits_tier:
      - ccf:tier:cpu-64gb
    recommended_quantization: q4_k_m
    license: proprietary
    status: active
    xref_ollama: "codestral:22b"
    cc_forge_notes: Check licensing before deployment

  # ==========================================================================
  # Embedding Models
  # ==========================================================================

  - id: ccf:model:nomic-embed-text
    label: nomic-embed-text
    description: Text embedding model for RAG applications
    member_of_family: ccf:family:nomic
    has_capability:
      - ccf:capability:embedding
    parameter_count: "137M"
    context_length: 8192
    embedding_dimensions: 768
    fits_tier:
      - ccf:tier:gpu-8gb
      - ccf:tier:cpu-64gb
    license: apache-2.0
    status: active
    xref_ollama: nomic-embed-text
    cc_forge_notes: Primary embedding model for KB RAG
