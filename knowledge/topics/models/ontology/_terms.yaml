# CC Forge Model Ontology Terms
# Version: 0.1.0
# Updated: 2026-01-06

version: "0.1.0"

# =============================================================================
# FAMILIES - Model lineages
# =============================================================================

families:
  - id: ccf:family:qwen
    label: "Qwen"
    definition: "Family of large language models developed by Alibaba Cloud's Qwen Team"
    developed_by: "Alibaba Cloud"
    homepage: "https://qwenlm.github.io/"
    xrefs:
      huggingface: "Qwen"
      github: "QwenLM"

  - id: ccf:family:deepseek
    label: "DeepSeek"
    definition: "Family of models from DeepSeek AI, known for MoE architectures and reasoning"
    developed_by: "DeepSeek AI"
    homepage: "https://www.deepseek.com/"
    xrefs:
      huggingface: "deepseek-ai"
      github: "deepseek-ai"

  - id: ccf:family:llama
    label: "Llama"
    definition: "Meta's open-weights large language model family"
    developed_by: "Meta AI"
    homepage: "https://ai.meta.com/"
    xrefs:
      huggingface: "meta-llama"

  - id: ccf:family:mistral
    label: "Mistral"
    definition: "Efficient models from Mistral AI, known for sliding window attention"
    developed_by: "Mistral AI"
    homepage: "https://mistral.ai/"
    xrefs:
      huggingface: "mistralai"

  - id: ccf:family:stable-diffusion
    label: "Stable Diffusion"
    definition: "Open-source text-to-image diffusion models from Stability AI"
    developed_by: "Stability AI"
    homepage: "https://stability.ai/"
    xrefs:
      huggingface: "stabilityai"

# =============================================================================
# CAPABILITIES - What models can do
# =============================================================================

capabilities:
  # Top-level
  - id: ccf:capability:text-generation
    label: "Text Generation"
    definition: "Generating text output from text input"

  - id: ccf:capability:embedding
    label: "Embedding"
    definition: "Converting text to vector representations"

  - id: ccf:capability:image-generation
    label: "Image Generation"
    definition: "Generating images from text or image input"

  # Text generation subtypes
  - id: ccf:capability:code-generation
    label: "Code Generation"
    definition: "Generating source code from natural language descriptions"
    is_a: ccf:capability:text-generation

  - id: ccf:capability:code-completion
    label: "Code Completion"
    definition: "Fill-in-middle (FIM) code completion for IDE integration"
    is_a: ccf:capability:code-generation

  - id: ccf:capability:reasoning
    label: "Reasoning"
    definition: "Step-by-step logical reasoning and problem solving"
    is_a: ccf:capability:text-generation

  - id: ccf:capability:reasoning-cot
    label: "Chain-of-Thought Reasoning"
    definition: "Extended reasoning with visible thinking process"
    is_a: ccf:capability:reasoning

  - id: ccf:capability:reasoning-math
    label: "Mathematical Reasoning"
    definition: "Solving mathematical problems with formal reasoning"
    is_a: ccf:capability:reasoning

  - id: ccf:capability:chat
    label: "Chat/Conversation"
    definition: "Multi-turn conversational interaction"
    is_a: ccf:capability:text-generation

  - id: ccf:capability:instruction-following
    label: "Instruction Following"
    definition: "Following complex multi-step instructions"
    is_a: ccf:capability:text-generation

  - id: ccf:capability:long-context
    label: "Long Context Processing"
    definition: "Processing documents with 32K+ token contexts"
    is_a: ccf:capability:text-generation

# =============================================================================
# TIERS - Hardware deployment tiers (CC Forge specific)
# =============================================================================

tiers:
  - id: ccf:tier:gpu-8gb
    label: "Tier 1: GPU (8GB VRAM)"
    definition: "Intel Arc or similar with ~8GB VRAM"
    constraints:
      max_model_size: "~14B at Q4_K_M"
      speed: "fast"
    cc_forge_service: "ollama-ipex"

  - id: ccf:tier:cpu-64gb
    label: "Tier 2: CPU (64GB RAM)"
    definition: "CPU inference with 64GB+ system RAM"
    constraints:
      max_model_size: "~70B at Q4_K_M"
      speed: "slow"
    cc_forge_service: "ollama-cpu"

  - id: ccf:tier:api
    label: "Tier 3: External API"
    definition: "External API when local is insufficient"
    constraints:
      max_model_size: "unlimited"
      speed: "variable"

# =============================================================================
# QUANTIZATION - Precision levels
# =============================================================================

quantization:
  - id: ccf:quant:fp16
    label: "FP16"
    definition: "16-bit floating point (full precision)"
    bits: 16
    quality_ratio: 1.0
    size_ratio: 1.0

  - id: ccf:quant:q8_0
    label: "Q8_0"
    definition: "8-bit quantization"
    bits: 8
    quality_ratio: 0.98
    size_ratio: 0.5

  - id: ccf:quant:q6_k
    label: "Q6_K"
    definition: "6-bit K-quant"
    bits: 6
    quality_ratio: 0.95
    size_ratio: 0.38

  - id: ccf:quant:q5_k_m
    label: "Q5_K_M"
    definition: "5-bit K-quant medium"
    bits: 5
    quality_ratio: 0.93
    size_ratio: 0.31

  - id: ccf:quant:q4_k_m
    label: "Q4_K_M"
    definition: "4-bit K-quant medium - recommended default"
    bits: 4
    quality_ratio: 0.90
    size_ratio: 0.25
    recommended: true

  - id: ccf:quant:q3_k_m
    label: "Q3_K_M"
    definition: "3-bit K-quant medium - quality degradation visible"
    bits: 3
    quality_ratio: 0.85
    size_ratio: 0.19

# =============================================================================
# MODELS - Specific model variants
# =============================================================================

models:
  # Qwen Coding Models
  - id: ccf:model:qwen2.5-coder-7b
    label: "Qwen2.5-Coder-7B-Instruct"
    definition: "7B code-specialized model from Qwen 2.5 series"
    is_a: ccf:family:qwen
    has_capability:
      - ccf:capability:code-generation
      - ccf:capability:code-completion
      - ccf:capability:instruction-following
    parameter_count: "7B"
    context_length: 131072
    fits_tier:
      - ccf:tier:gpu-8gb   # at Q4_K_M or Q8_0
    license: "Apache-2.0"
    xrefs:
      ollama: "qwen2.5-coder:7b-instruct"
      huggingface: "Qwen/Qwen2.5-Coder-7B-Instruct"
    cc_forge_notes: "Primary coding model for Tier 1"

  - id: ccf:model:qwen2.5-coder-32b
    label: "Qwen2.5-Coder-32B-Instruct"
    definition: "32B code-specialized model, best open-source coder"
    is_a: ccf:family:qwen
    has_capability:
      - ccf:capability:code-generation
      - ccf:capability:code-completion
      - ccf:capability:instruction-following
    parameter_count: "32B"
    context_length: 131072
    fits_tier:
      - ccf:tier:cpu-64gb
    license: "Apache-2.0"
    xrefs:
      ollama: "qwen2.5-coder:32b-instruct"
      huggingface: "Qwen/Qwen2.5-Coder-32B-Instruct"
    cc_forge_notes: "Use when 7B produces incorrect code"

  # Qwen Reasoning Models
  - id: ccf:model:qwq-32b
    label: "QwQ-32B"
    definition: "32B reasoning model with chain-of-thought capability"
    is_a: ccf:family:qwen
    has_capability:
      - ccf:capability:reasoning-cot
      - ccf:capability:reasoning-math
      - ccf:capability:code-generation
    parameter_count: "32B"
    context_length: 32768
    fits_tier:
      - ccf:tier:cpu-64gb
    license: "Apache-2.0"
    xrefs:
      ollama: "qwq:32b"
      huggingface: "Qwen/QwQ-32B"
    cc_forge_notes: "Best reasoning at manageable size"

  # DeepSeek Models
  - id: ccf:model:deepseek-r1-7b
    label: "DeepSeek-R1-Distill-7B"
    definition: "7B distilled reasoning model from DeepSeek R1"
    is_a: ccf:family:deepseek
    derived_from: ccf:model:deepseek-r1
    has_capability:
      - ccf:capability:reasoning-cot
      - ccf:capability:reasoning-math
    parameter_count: "7B"
    context_length: 65536
    fits_tier:
      - ccf:tier:gpu-8gb
    license: "MIT"
    xrefs:
      ollama: "deepseek-r1:7b"
      huggingface: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
    cc_forge_notes: "Reasoning capability at Tier 1 size"

  - id: ccf:model:deepseek-r1
    label: "DeepSeek-R1"
    definition: "671B MoE reasoning model rivaling OpenAI o1"
    is_a: ccf:family:deepseek
    has_capability:
      - ccf:capability:reasoning-cot
      - ccf:capability:reasoning-math
      - ccf:capability:code-generation
    parameter_count: "671B (37B active)"
    context_length: 131072
    fits_tier:
      - ccf:tier:api  # Too large for local
    license: "MIT"
    xrefs:
      huggingface: "deepseek-ai/DeepSeek-R1"

  # Llama Models
  - id: ccf:model:llama-3.1-8b
    label: "Llama-3.1-8B-Instruct"
    definition: "8B general-purpose model from Meta"
    is_a: ccf:family:llama
    has_capability:
      - ccf:capability:chat
      - ccf:capability:instruction-following
      - ccf:capability:long-context
    parameter_count: "8B"
    context_length: 131072
    fits_tier:
      - ccf:tier:gpu-8gb
    license: "Llama 3.1 Community"
    xrefs:
      ollama: "llama3.1:8b-instruct"
      huggingface: "meta-llama/Llama-3.1-8B-Instruct"
    cc_forge_notes: "Good general-purpose baseline"

  - id: ccf:model:llama-3.3-70b
    label: "Llama-3.3-70B-Instruct"
    definition: "70B model with 405B-level quality"
    is_a: ccf:family:llama
    has_capability:
      - ccf:capability:chat
      - ccf:capability:instruction-following
      - ccf:capability:long-context
    parameter_count: "70B"
    context_length: 131072
    fits_tier:
      - ccf:tier:cpu-64gb
    license: "Llama 3.3 Community"
    xrefs:
      ollama: "llama3.3:70b-instruct"
      huggingface: "meta-llama/Llama-3.3-70B-Instruct"
    cc_forge_notes: "Best quality when you can wait"

  # Embedding Models
  - id: ccf:model:nomic-embed-text
    label: "nomic-embed-text"
    definition: "Text embedding model for RAG applications"
    has_capability:
      - ccf:capability:embedding
    parameter_count: "137M"
    context_length: 8192
    embedding_dimensions: 768
    fits_tier:
      - ccf:tier:gpu-8gb
      - ccf:tier:cpu-64gb
    license: "Apache-2.0"
    xrefs:
      ollama: "nomic-embed-text"
    cc_forge_notes: "Primary embedding model for KB RAG"
